{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Advanced Ollama Setup\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In this notebook, we'll set up multiple Ollama instances in Docker containers. Ollama is an open-source tool that allows us to run large language models locally. This setup will enable us to use different models for various tasks in our RAG (Retrieval-Augmented Generation) system, such as embedding generation and text generation.\n",
    "\n",
    "We'll cover the following steps:\n",
    "1. Updating our project directory structure\n",
    "2. Creating an OllamaManager class to handle Ollama operations\n",
    "3. Updating our environment variables\n",
    "4. Updating our Docker Compose configuration\n",
    "5. Testing the OllamaManager class\n",
    "\n",
    "## 2. Update Project Directory Structure\n",
    "\n",
    "First, let's update our project directory structure to accommodate the Ollama models and ensure they're shared between containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the project root directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Create directories for Ollama models\n",
    "ollama_models_path = os.path.join(project_root, 'db_data', 'ollama_models')\n",
    "ollama_llm_path = os.path.join(ollama_models_path, 'llm')\n",
    "\n",
    "os.makedirs(ollama_models_path, exist_ok=True)\n",
    "os.makedirs(ollama_llm_path, exist_ok=True)\n",
    "\n",
    "print(f\"Created Ollama models directory at: {ollama_models_path}\")\n",
    "print(f\"Created Ollama LLM directory at: {ollama_llm_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our updated project structure now looks like this:\n",
    "\n",
    "```\n",
    "RAG_tools/\n",
    "├── config/\n",
    "│   ├── docker-compose.yml\n",
    "│   └── .env\n",
    "├── notebooks/\n",
    "│   ├── 00_Environment_Setup.ipynb\n",
    "│   ├── 01_Database_Setup.ipynb\n",
    "│   └── 02_Ollama_setup.ipynb\n",
    "├── src/\n",
    "│   └── utils/\n",
    "│       ├── config_utils.py\n",
    "│       └── ollama_manager.py\n",
    "├── db_data/\n",
    "│   ├── postgres/\n",
    "│   ├── neo4j/\n",
    "│   └── ollama_models/\n",
    "│       └── llm/\n",
    "└── tests/\n",
    "```\n",
    "\n",
    "## 3. Create OllamaManager Class\n",
    "\n",
    "Now, let's create the OllamaManager class. This class will be used to launch and manage Ollama instances in Docker containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/utils/ollama_manager.py\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "from .DockerComposeManager import DockerComposeManager\n",
    "from .config_utils import Config\n",
    "\n",
    "class OllamaManager:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.container_name = self.config.OLLAMA_LLM_CONTAINER_NAME\n",
    "        self.port = self.config.OLLAMA_LLM_PORT\n",
    "        self.model = self.config.OLLAMA_LLM_MODEL\n",
    "        self.gpu = self.config.OLLAMA_LLM_GPU\n",
    "        \n",
    "        self.models_path = self.config.OLLAMA_MODELS_PATH or os.path.expanduser('~/ollama_models')\n",
    "        self.llm_path = self.config.OLLAMA_LLM_PATH or os.path.join(self.models_path, 'llm')\n",
    "        \n",
    "        # Ensure directories exist\n",
    "        os.makedirs(self.models_path, exist_ok=True)\n",
    "        os.makedirs(self.llm_path, exist_ok=True)\n",
    "        \n",
    "        # Initialize DockerComposeManager\n",
    "        docker_compose_path = os.path.join('..', 'config', 'docker-compose.yml')\n",
    "        self.docker_manager = DockerComposeManager(docker_compose_path)\n",
    "\n",
    "        logging.info(f\"OllamaManager initialized with models_path: {self.models_path}, llm_path: {self.llm_path}\")\n",
    "        logging.info(f\"Using model: {self.model} on port: {self.port}\")\n",
    "\n",
    "    def ensure_model_available(self):\n",
    "        if not self.is_model_running():\n",
    "            logging.info(f\"Model {self.model} not found. Attempting to download...\")\n",
    "            self.pull_model()\n",
    "        else:\n",
    "            logging.info(f\"Model {self.model} is already available.\")\n",
    "\n",
    "    def generate_response(self, prompt):\n",
    "        self.ensure_model_available()\n",
    "        try:\n",
    "            payload = {\n",
    "                'model': self.model,\n",
    "                'prompt': prompt\n",
    "            }\n",
    "            logging.debug(f\"Sending request to Ollama API with payload: {payload}\")\n",
    "            logging.debug(f\"API URL: http://localhost:{self.port}/api/generate\")\n",
    "            \n",
    "            response = requests.post(\n",
    "                f'http://localhost:{self.port}/api/generate',\n",
    "                json=payload,\n",
    "                stream=True\n",
    "            )\n",
    "            logging.debug(f\"Response status code: {response.status_code}\")\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            full_response = \"\"\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        chunk = json.loads(line)\n",
    "                        logging.debug(f\"Received chunk: {chunk}\")\n",
    "                        if 'response' in chunk:\n",
    "                            token = chunk['response']\n",
    "                            full_response += token\n",
    "                            print(token, end='', flush=True)\n",
    "                        if chunk.get('done', False):\n",
    "                            break\n",
    "                    except json.JSONDecodeError:\n",
    "                        logging.warning(f\"Failed to decode JSON: {line}\")\n",
    "            \n",
    "            print(\"\\n\")  # New line after the response\n",
    "            return full_response.strip()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error generating response: {str(e)}\")\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                logging.error(f\"Response content: {e.response.text}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {str(e)}\")\n",
    "            return f\"Unexpected error: {str(e)}\"\n",
    "\n",
    "    def is_model_running(self):\n",
    "        try:\n",
    "            response = requests.get(f'http://localhost:{self.port}/api/tags')\n",
    "            response.raise_for_status()\n",
    "            models = response.json()\n",
    "            logging.debug(f\"Available models: {models}\")\n",
    "            return self.model in [model['name'] for model in models['models']]\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error checking if model is running: {e}\")\n",
    "            return False\n",
    "\n",
    "    def pull_model(self):\n",
    "        logging.info(f\"Pulling model {self.model}...\")\n",
    "        try:\n",
    "            response = requests.post(f'http://localhost:{self.port}/api/pull', json={'name': self.model}, stream=True)\n",
    "            response.raise_for_status()\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    print(line.decode())\n",
    "            logging.info(f\"Successfully pulled model {self.model}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error pulling model: {str(e)}\")\n",
    "            raise RuntimeError(f\"Failed to download model {self.model}. Error: {str(e)}\")\n",
    "\n",
    "    def start_container(self):\n",
    "        self.docker_manager.start_containers()\n",
    "        logging.info(f\"Started container: {self.container_name}\")\n",
    "        self.wait_for_ollama()\n",
    "        self.ensure_model_available()\n",
    "\n",
    "    def stop_container(self):\n",
    "        self.docker_manager.stop_containers()\n",
    "        logging.info(f\"Stopped container: {self.container_name}\")\n",
    "\n",
    "    def wait_for_ollama(self, max_attempts=5, delay=5):\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                response = requests.get(f'http://localhost:{self.port}/api/tags')\n",
    "                if response.status_code == 200:\n",
    "                    logging.info(f\"Successfully connected to Ollama on port {self.port}\")\n",
    "                    return True\n",
    "            except requests.exceptions.RequestException:\n",
    "                logging.warning(f\"Attempt {attempt + 1}/{max_attempts}: Ollama on port {self.port} is not ready yet. Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "        logging.error(f\"Failed to connect to Ollama after {max_attempts} attempts\")\n",
    "        raise RuntimeError(f\"Failed to connect to Ollama after {max_attempts} attempts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Update Environment Variables\n",
    "\n",
    "Now, let's update our .env file to include the Ollama-related variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "env_file_path = os.path.join('..', 'config', '.env')\n",
    "\n",
    "ollama_env_vars = \"\"\"\n",
    "# Ollama Configuration\n",
    "OLLAMA_EMBEDDING_CONTAINER_NAME=ragtools_ollama_embedding\n",
    "OLLAMA_EMBEDDING_PORT=12434\n",
    "OLLAMA_EMBEDDING_MODEL=bert-base-multilingual-cased\n",
    "OLLAMA_EMBEDDING_GPU=0\n",
    "\n",
    "OLLAMA_LLM_CONTAINER_NAME=ragtools_ollama_llm\n",
    "OLLAMA_LLM_PORT=12435\n",
    "OLLAMA_LLM_MODEL=tinyllama\n",
    "OLLAMA_LLM_GPU=1\n",
    "\n",
    "OLLAMA_MODELS_PATH=./db_data/ollama_models\n",
    "OLLAMA_LLM_PATH=./db_data/ollama_models/llm\n",
    "\"\"\"\n",
    "\n",
    "with open(env_file_path, 'a') as f:\n",
    "    f.write(ollama_env_vars)\n",
    "\n",
    "print(\"Updated .env file with Ollama configurations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Update Config Class\n",
    "\n",
    "Before we proceed with the verification step, we need to update our Config class to include the new Ollama-related attributes. This is a crucial step when extending our framework with new components.\n",
    "\n",
    "This step demonstrates how to extend the Config class when new components are added to the framework. It's important to update this class whenever new environment variables or configuration options are introduced.\n",
    "\n",
    "Let's update the `config_utils.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "config_utils_path = os.path.join('..', 'src', 'utils', 'config_utils.py')\n",
    "\n",
    "# Read the existing content\n",
    "with open(config_utils_path, 'r') as f:\n",
    "    existing_content = f.read()\n",
    "\n",
    "# Define the new Ollama configurations\n",
    "ollama_configs = '''        # Ollama configurations\n",
    "        self.OLLAMA_EMBEDDING_CONTAINER_NAME = os.getenv('OLLAMA_EMBEDDING_CONTAINER_NAME')\n",
    "        self.OLLAMA_EMBEDDING_PORT = int(os.getenv('OLLAMA_EMBEDDING_PORT', 12434))\n",
    "        self.OLLAMA_EMBEDDING_MODEL = os.getenv('OLLAMA_EMBEDDING_MODEL')\n",
    "        self.OLLAMA_EMBEDDING_GPU = int(os.getenv('OLLAMA_EMBEDDING_GPU', 0))\n",
    "        self.OLLAMA_LLM_CONTAINER_NAME = os.getenv('OLLAMA_LLM_CONTAINER_NAME')\n",
    "        self.OLLAMA_LLM_PORT = int(os.getenv('OLLAMA_LLM_PORT', 12435))\n",
    "        self.OLLAMA_LLM_MODEL = os.getenv('OLLAMA_LLM_MODEL')\n",
    "        self.OLLAMA_LLM_GPU = int(os.getenv('OLLAMA_LLM_GPU', 1))\n",
    "        self.OLLAMA_MODELS_PATH = os.getenv('OLLAMA_MODELS_PATH')\n",
    "        self.OLLAMA_LLM_PATH = os.getenv('OLLAMA_LLM_PATH')\n",
    "'''\n",
    "\n",
    "# Find the position to insert the new configurations\n",
    "lines = existing_content.split('\\n')\n",
    "insert_line = -1\n",
    "for i, line in enumerate(lines):\n",
    "    if line.strip().startswith('def get_postgres_connection_params(self):'):\n",
    "        insert_line = i\n",
    "        break\n",
    "\n",
    "if insert_line == -1:\n",
    "    # If method not found, insert at the end of __init__\n",
    "    for i, line in enumerate(reversed(lines)):\n",
    "        if line.strip() == \"self.DOCKER_NETWORK_NAME = os.getenv('DOCKER_NETWORK_NAME')\":\n",
    "            insert_line = len(lines) - i\n",
    "            break\n",
    "\n",
    "# Insert the new configurations\n",
    "if insert_line != -1:\n",
    "    updated_lines = lines[:insert_line] + ollama_configs.split('\\n') + lines[insert_line:]\n",
    "    updated_content = '\\n'.join(updated_lines)\n",
    "else:\n",
    "    print(\"Could not find appropriate insertion point. Please update manually.\")\n",
    "    updated_content = existing_content\n",
    "\n",
    "# Write the updated content back to the file\n",
    "with open(config_utils_path, 'w') as f:\n",
    "    f.write(updated_content)\n",
    "\n",
    "print(\"Updated config_utils.py with Ollama configurations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Update Docker Compose Configuration\n",
    "\n",
    "Let's update our docker-compose.yml file to include the Ollama services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "docker_compose_path = os.path.join('..', 'config', 'docker-compose.yml')\n",
    "\n",
    "# Read existing docker-compose.yml\n",
    "with open(docker_compose_path, 'r') as f:\n",
    "    docker_compose = yaml.safe_load(f)\n",
    "\n",
    "# Add or update Ollama services\n",
    "docker_compose['services']['ollama_embedding'] = {\n",
    "    'image': 'ollama/ollama',\n",
    "    'container_name': '${OLLAMA_EMBEDDING_CONTAINER_NAME}',\n",
    "    'environment': ['OLLAMA_HOST=0.0.0.0:${OLLAMA_EMBEDDING_PORT}'],\n",
    "    'ports': ['${OLLAMA_EMBEDDING_PORT}:${OLLAMA_EMBEDDING_PORT}'],\n",
    "    'volumes': ['${OLLAMA_MODELS_PATH}:/root/.ollama'],\n",
    "    'deploy': {\n",
    "        'resources': {\n",
    "            'reservations': {\n",
    "                'devices': [{'driver': 'nvidia', 'count': 1, 'capabilities': ['gpu']}]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'networks': ['ragtools_network']\n",
    "}\n",
    "\n",
    "docker_compose['services']['ollama_llm'] = {\n",
    "    'image': 'ollama/ollama',\n",
    "    'container_name': '${OLLAMA_LLM_CONTAINER_NAME}',\n",
    "    'environment': ['OLLAMA_HOST=0.0.0.0:${OLLAMA_LLM_PORT}'],\n",
    "    'ports': ['${OLLAMA_LLM_PORT}:${OLLAMA_LLM_PORT}'],\n",
    "    'volumes': [\n",
    "        '${OLLAMA_MODELS_PATH}:/root/.ollama',\n",
    "        '${OLLAMA_LLM_PATH}:/root/.ollama/llm'\n",
    "    ],\n",
    "    'deploy': {\n",
    "        'resources': {\n",
    "            'reservations': {\n",
    "                'devices': [{'driver': 'nvidia', 'count': 1, 'capabilities': ['gpu']}]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'networks': ['ragtools_network']\n",
    "}\n",
    "\n",
    "# Ensure the network is defined\n",
    "if 'networks' not in docker_compose:\n",
    "    docker_compose['networks'] = {}\n",
    "docker_compose['networks']['ragtools_network'] = {'name': '${DOCKER_NETWORK_NAME}'}\n",
    "\n",
    "# Write updated docker-compose.yml\n",
    "with open(docker_compose_path, 'w') as f:\n",
    "    yaml.dump(docker_compose, f)\n",
    "\n",
    "print(\"Updated docker-compose.yml with Ollama services.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test OllamaManager Class\n",
    "\n",
    "Now, let's test our OllamaManager class by spinning up a test model and running a simple prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from src.utils.config_utils import Config\n",
    "from src.utils.ollama_manager import OllamaManager\n",
    "from src.utils.DockerComposeManager import DockerComposeManager\n",
    "\n",
    "# Initialize the Config class\n",
    "config = Config()\n",
    "\n",
    "# Print all configuration values for debugging\n",
    "print(\"Configuration values:\")\n",
    "for attr, value in vars(config).items():\n",
    "    print(f\"{attr}: {value}\")\n",
    "\n",
    "# Initialize DockerComposeManager\n",
    "docker_compose_path = os.path.join(project_root, 'config', 'docker-compose.yml')\n",
    "\n",
    "# Initialize OllamaManager for the embedding model\n",
    "try:\n",
    "    embedding_manager = OllamaManager(\n",
    "        container_name=config.OLLAMA_EMBEDDING_0_CONTAINER_NAME,\n",
    "        port=config.OLLAMA_EMBEDDING_0_PORT,\n",
    "        model=config.OLLAMA_EMBEDDING_0_MODEL,\n",
    "        gpu=config.OLLAMA_EMBEDDING_0_GPU,\n",
    "        models_path=config.OLLAMA_MODELS_PATH,\n",
    "        llm_path=config.OLLAMA_LLM_PATH,\n",
    "        docker_compose_path=docker_compose_path\n",
    "    )\n",
    "\n",
    "    # Initialize OllamaManager for the Codestral model\n",
    "    codestral_manager = OllamaManager(\n",
    "        container_name=config.OLLAMA_CODESTRAL_CONTAINER_NAME,\n",
    "        port=config.OLLAMA_CODESTRAL_PORT,\n",
    "        model=config.OLLAMA_CODESTRAL_MODEL,\n",
    "        gpu=config.OLLAMA_CODESTRAL_GPU,\n",
    "        models_path=config.OLLAMA_MODELS_PATH,\n",
    "        llm_path=config.OLLAMA_LLM_PATH,\n",
    "        docker_compose_path=docker_compose_path\n",
    "    )\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error initializing OllamaManager: {str(e)}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Start the containers\n",
    "print(\"Starting Ollama containers...\")\n",
    "try:\n",
    "    embedding_manager.start_container()\n",
    "    codestral_manager.start_container()\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error starting containers: {str(e)}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Wait for containers to start\n",
    "time.sleep(10)\n",
    "\n",
    "# Check container status\n",
    "print(\"\\nChecking container status:\")\n",
    "docker_manager = DockerComposeManager(docker_compose_path)\n",
    "docker_manager.show_container_status()\n",
    "\n",
    "# Verify that models are available\n",
    "print(\"\\nVerifying model availability:\")\n",
    "embedding_manager.ensure_model_available()\n",
    "codestral_manager.ensure_model_available()\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is the capital of Texas?\",\n",
    "    \"Write a Python function to calculate the Fibonacci sequence.\"\n",
    "]\n",
    "\n",
    "# Test the codestral model\n",
    "print(\"\\nTesting codestral model:\")\n",
    "for question in test_questions:\n",
    "    print(f\"\\nPrompt: {question}\")\n",
    "    response = codestral_manager.generate_response(question)\n",
    "    print(f\"Response: {response}\")\n",
    "\n",
    "# Test the embedding model (if applicable)\n",
    "# Note: This is just a placeholder. Adjust based on your embedding model's capabilities.\n",
    "print(\"\\nTesting Embedding model:\")\n",
    "embedding_text = \"This is a test sentence for embedding.\"\n",
    "print(f\"Embedding text: {embedding_text}\")\n",
    "# Uncomment and adjust the following line based on your embedding model's interface\n",
    "# embedding = embedding_manager.generate_embedding(embedding_text)\n",
    "# print(f\"Embedding (first 5 dimensions): {embedding[:5]}\")\n",
    "\n",
    "# Stop the containers\n",
    "print(\"\\nStopping Ollama containers...\")\n",
    "embedding_manager.stop_container()\n",
    "codestral_manager.stop_container()\n",
    "\n",
    "print(\"\\nVerification complete. All containers have been stopped.\")\n",
    "\n",
    "# Additional debug information\n",
    "print(\"\\nDebug Information:\")\n",
    "print(f\"OLLAMA_EMBEDDING_0_PORT: {config.OLLAMA_EMBEDDING_0_PORT}\")\n",
    "print(f\"OLLAMA_EMBEDDING_0_MODEL: {config.OLLAMA_EMBEDDING_0_MODEL}\")\n",
    "print(f\"OLLAMA_CODESTRAL_PORT: {config.OLLAMA_CODESTRAL_PORT}\")\n",
    "print(f\"OLLAMA_CODESTRAL_MODEL: {config.OLLAMA_CODESTRAL_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have successfully:\n",
    "\n",
    "1. Set up Ollama instances in Docker containers\n",
    "2. Created an OllamaManager class to handle Ollama operations\n",
    "3. Implemented a method to generate responses from the LLM\n",
    "4. Demonstrated the streaming nature of the LLM's output\n",
    "5. Verified the functionality of our setup with test questions\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Our next notebook will focus on creating a CLI interface for interacting with the LLM. Before diving into the implementation, we'll need to consider:\n",
    "\n",
    "1. LLM Configurables:\n",
    "   - Context length\n",
    "   - Temperature\n",
    "   - Other relevant parameters (e.g., top_p, frequency_penalty, presence_penalty)\n",
    "\n",
    "2. CLI Interface Options:\n",
    "   - Evaluate the merits of adopting a pre-built CLI interface vs. creating our own\n",
    "   - Consider libraries like `click`, `typer`, or `argparse` for building a custom CLI\n",
    "\n",
    "3. Chat Interface Design:\n",
    "   - How to maintain conversation history\n",
    "   - Handling user input and system responses\n",
    "   - Implementing commands for adjusting LLM parameters on-the-fly\n",
    "\n",
    "4. Integration with OllamaManager:\n",
    "   - How to incorporate our existing OllamaManager class into the CLI interface\n",
    "\n",
    "By addressing these points, we'll be well-prepared to create a robust and user-friendly CLI for interacting with our LLM setup."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragtools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
