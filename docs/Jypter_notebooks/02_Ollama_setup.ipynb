{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Advanced Ollama Setup\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In this notebook, we'll set up multiple Ollama instances in Docker containers. Ollama is an open-source tool that allows us to run large language models locally. This setup will enable us to use different models for various tasks in our RAG (Retrieval-Augmented Generation) system, such as embedding generation and text generation.\n",
    "\n",
    "We'll cover the following steps:\n",
    "1. Updating our project directory structure\n",
    "2. Creating an OllamaManager class to handle Ollama operations\n",
    "3. Updating our environment variables\n",
    "4. Updating our Docker Compose configuration\n",
    "5. Testing the OllamaManager class\n",
    "\n",
    "## 2. Update Project Directory Structure\n",
    "\n",
    "First, let's update our project directory structure to accommodate the Ollama models and ensure they're shared between containers.\n",
    "\n",
    "Our updated project structure will now looks like this:\n",
    "\n",
    "```\n",
    "ProjectName/\n",
    "├── config/\n",
    "│   ├── docker-compose.yml\n",
    "│   └── .env\n",
    "├── notebooks/\n",
    "│   ├── 00_Environment_Setup.ipynb\n",
    "│   ├── 01_Database_Setup.ipynb\n",
    "│   └── 02_Ollama_setup.ipynb\n",
    "├── src/\n",
    "│   └── utils/\n",
    "│       ├── config_utils.py\n",
    "│       └── ollama_manager.py\n",
    "├── db_data/\n",
    "│   ├── postgres/\n",
    "│   ├── neo4j/\n",
    "│   └── ollama_models/\n",
    "│       └── llm/\n",
    "└── tests/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the project root directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "\n",
    "# Create directories for Ollama models\n",
    "ollama_models_path = os.path.join(project_root, 'db_data', OLLAMA_models')\n",
    "os.makedirs(ollama_models_path, exist_ok=True)\n",
    "\n",
    "print(f\"Created Ollama models directory at: {ollama_models_path}\")\n",
    "\n",
    "# Instead of creating a specific CodeLlama directory, we'll create a function to make model-specific directories as we need them.\n",
    "def create_model_directory(model_name):\n",
    "    model_path = os.path.join(ollama_models_path, model_name)\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    print(f\"Created {model_name} directory at: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This setup provides a flexible structure for managing multiple Ollama models. The `create_model_directory` function will be used later in this notebook for the OllamaManager class.\n",
    "\n",
    "**Example usage:**\n",
    "```python\n",
    "create_model_directory('codellama')\n",
    "```\n",
    "\n",
    "Explanation of changes:\n",
    "- We've made the directory creation more generic to support multiple models.\n",
    "- We've created a function `create_model_directory` to easily create directories for new models.\n",
    "\n",
    "The `create_model_directory` function takes a model name as an argument and creates a dedicated directory for that model within the `ollama_models_path`. This allows us to organize model-specific data and configurations separately, which will be crucial when working with multiple models in our RAG system.\n",
    "\n",
    "## 3. Update Environment Variables\n",
    "\n",
    "Now, let's update our .env file with the necessary Ollama configurations. These environment variables will serve as the foundation for our OllamaManager class and Docker setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ollama environment variables to append\n",
    "ollama_env_vars = \"\"\"\n",
    "# Ollama Configuration\n",
    "OLLAMA_MODELS_PATH=./db_data/ollama_models\n",
    "\n",
    "# Ollama Configuration\n",
    "OLLAMA_MODELS_PATH=./db_data/ollama_models\n",
    "\n",
    "# CodeLlama Configuration\n",
    "OLLAMA_CODESTRALL_CONTAINER_NAME=ragtools_ollama_codestral\n",
    "OLLAMA_CODESTRALL_PORT=11435\n",
    "OLLAMA_CODESTRALL_MODEL=sammcj/codestral-tweaked-22b\n",
    "OLLAMA_CODESTRALL_PATH=../db_data/ollama_models/codestral\n",
    "OLLAMA_CODESTRALL_GPU=0\n",
    "OLLAMA_CODESTRALL_HOST=0.0.0.0\n",
    "\n",
    "# Example of another model configuration (commented out)\n",
    "# OLLAMA_LLAMA2_CONTAINER_NAME=ragtools_ollama_llama2\n",
    "# OLLAMA_LLAMA2_PORT=11436\n",
    "# OLLAMA_LLAMA2_MODEL=llama2\n",
    "# OLLAMA_LLAMA2_PATH=./db_data/ollama_models/llama2\n",
    "# OLLAMA_LLAMA2_GPU=1  # Assign to second GPU\n",
    "\"\"\"\n",
    "\n",
    "# Path to the .env file\n",
    "env_file_path = os.path.join('..', '..', 'config', '.env')\n",
    "\n",
    "try:\n",
    "    # Check if file exists and is writable\n",
    "    if os.path.exists(env_file_path):\n",
    "        if os.access(env_file_path, os.W_OK):\n",
    "            with open(env_file_path, 'a') as f:\n",
    "                f.write(ollama_env_vars)\n",
    "            logger.info(f\"Successfully appended Ollama configurations to .env file at {env_file_path}\")\n",
    "        else:\n",
    "            logger.error(f\"The .env file at {env_file_path} is not writable.\")\n",
    "    else:\n",
    "        logger.error(f\".env file not found at {env_file_path}. Please create it first.\")\n",
    "\n",
    "except IOError as e:\n",
    "    logger.error(f\"An error occurred while writing to the .env file: {e}\")\n",
    "\n",
    "print(\"Ollama environment variable setup completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code appends the Ollama-specific configurations to our existing .env file. These variables will be used in our Docker Compose file to set up the Ollama services.\n",
    "\n",
    "## 4. Review Docker Compose Configuration\n",
    "\n",
    "Our docker-compose.yml file now includes the following Ollama service configuration:\n",
    "\n",
    "```yaml\n",
    "ollama_codestral:\n",
    "  image: ollama/ollama\n",
    "  container_name: ${OLLAMA_CODESTRALL_CONTAINER_NAME:-ragtools_ollama_codestral}\n",
    "  environment:\n",
    "    - OLLAMA_HOST=0.0.0.0:${OLLAMA_CODESTRALL_PORT:-11435}\n",
    "  env_file:\n",
    "    - .env\n",
    "  ports:\n",
    "    - ${OLLAMA_CODESTRALL_PORT:-11435}:${OLLAMA_CODESTRALL_PORT:-11435}\n",
    "  volumes:\n",
    "    - ../db_data/ollama_models:/root/.ollama\n",
    "    - ../db_data/ollama_models/codestral:/root/.ollama/codestral\n",
    "  entrypoint: [\"ollama\"]\n",
    "  command: [\"serve\"]\n",
    "  deploy:\n",
    "    resources:\n",
    "      reservations:\n",
    "        devices:\n",
    "          - driver: nvidia\n",
    "            count: 1\n",
    "            capabilities:\n",
    "              - gpu\n",
    "  networks:\n",
    "    - ragtools_network\n",
    "```\n",
    "\n",
    "This configuration ensures that:\n",
    "1. The Ollama service uses the correct port as specified in the .env file.\n",
    "2. The OLLAMA_HOST environment variable is set correctly.\n",
    "3. The service uses GPU capabilities if available.\n",
    "4. The correct volumes are mounted for persistent storage of models.\n",
    "\n",
    "**NOTE**: run the next code block to update the docker compose file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_service_config = \"\"\"\n",
    "    ollama_codestral:\n",
    "        image: ollama/ollama\n",
    "        container_name: ${OLLAMA_CODESTRALL_CONTAINER_NAME:-ragtools_ollama_codestral}\n",
    "        environment:\n",
    "            - OLLAMA_HOST=0.0.0.0:${OLLAMA_CODESTRALL_PORT:-11435}\n",
    "        env_file:\n",
    "            - .env\n",
    "        ports:\n",
    "            - ${OLLAMA_CODESTRALL_PORT:-11435}:${OLLAMA_CODESTRALL_PORT:-11435}\n",
    "        volumes:\n",
    "            - ../db_data/ollama_models:/root/.ollama\n",
    "            - ../db_data/ollama_models/codestral:/root/.ollama/codestral\n",
    "        entrypoint: [\"ollama\"]\n",
    "        command: [\"serve\"]\n",
    "        deploy:\n",
    "            resources:\n",
    "            reservations:\n",
    "                devices:\n",
    "                - driver: nvidia\n",
    "                    count: 1\n",
    "                    capabilities:\n",
    "                    - gpu\n",
    "        networks:\n",
    "            - ragtools_network\n",
    "\"\"\"\n",
    "\n",
    "print(\"Ollama service configuration in docker-compose.yml:\")\n",
    "print(ollama_service_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Update Config Class\n",
    "\n",
    "Before we proceed with the verification step, we need to update our Config class to include the new Ollama-related attributes. This is a crucial step when extending our framework with new components.\n",
    "\n",
    "This step demonstrates how to extend the Config class when new components are added to the framework. It's important to update this class whenever new environment variables or configuration options are introduced.\n",
    "\n",
    "Let's update the `config_utils.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "config_utils_path = os.path.join('..', 'src', 'utils', 'config_utils.py')\n",
    "\n",
    "# Read the existing content\n",
    "with open(config_utils_path, 'r') as f:\n",
    "    existing_content = f.read()\n",
    "\n",
    "# Define the new Ollama configurations\n",
    "ollama_configs = '''\n",
    "        # Ollama configurations\n",
    "        self.OLLAMA_MODELS_PATH = os.getenv('OLLAMA_MODELS_PATH')\n",
    "        \n",
    "        # CodeStral Configuration\n",
    "        self.OLLAMA_CODESTRALL_CONTAINER_NAME = os.getenv('OLLAMA_CODESTRALL_CONTAINER_NAME')\n",
    "        self.OLLAMA_CODESTRALL_PORT = int(os.getenv('OLLAMA_CODESTRALL_PORT', 11435))\n",
    "        self.OLLAMA_CODESTRALL_MODEL = os.getenv('OLLAMA_CODESTRALL_MODEL')\n",
    "        self.OLLAMA_CODESTRALL_PATH = os.getenv('OLLAMA_CODESTRALL_PATH')\n",
    "        self.OLLAMA_CODESTRALL_GPU = int(os.getenv('OLLAMA_CODESTRALL_GPU', 0))\n",
    "        self.OLLAMA_CODESTRALL_HOST = os.getenv('OLLAMA_CODESTRALL_HOST')\n",
    "'''\n",
    "\n",
    "# Find the position to insert the new configurations\n",
    "lines = existing_content.split('\\n')\n",
    "insert_line = -1\n",
    "for i, line in enumerate(lines):\n",
    "    if line.strip().startswith('def get_postgres_connection_params(self):'):\n",
    "        insert_line = i\n",
    "        break\n",
    "\n",
    "if insert_line == -1:\n",
    "    # If method not found, insert at the end of __init__\n",
    "    for i, line in enumerate(reversed(lines)):\n",
    "        if line.strip() == \"self.DOCKER_NETWORK_NAME = os.getenv('DOCKER_NETWORK_NAME')\":\n",
    "            insert_line = len(lines) - i\n",
    "            break\n",
    "\n",
    "# Insert the new configurations\n",
    "if insert_line != -1:\n",
    "    updated_lines = lines[:insert_line] + ollama_configs.split('\\n') + lines[insert_line:]\n",
    "    updated_content = '\\n'.join(updated_lines)\n",
    "else:\n",
    "    print(\"Could not find appropriate insertion point. Please update manually.\")\n",
    "    updated_content = existing_content\n",
    "\n",
    "# Write the updated content back to the file\n",
    "with open(config_utils_path, 'w') as f:\n",
    "    f.write(updated_content)\n",
    "\n",
    "print(\"Updated config_utils.py with Ollama configurations.\")\n",
    "\n",
    "# Optionally, print the updated Config class for verification\n",
    "print(\"\\nUpdated Config class:\")\n",
    "print(updated_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ## Testing Ollama Setup\n",
    "\n",
    "Now that we have configured our Ollama service in the Docker Compose file and added an entrypoint script, let's test it to ensure everything is working correctly. For these steps, you'll need to open a terminal or command prompt and navigate to the directory containing your docker-compose.yml file.\n",
    "\n",
    "### 1. Ensure you're in the correct directory\n",
    "\n",
    "First, make sure you're in the directory containing your docker-compose.yml file:\n",
    "\n",
    "```bash\n",
    "cd path/to/your/project/config\n",
    "```\n",
    "\n",
    "### 2. Start the Docker containers:\n",
    "\n",
    "Run the following command to start your Docker containers:\n",
    "\n",
    "```bash\n",
    "docker compose --env-file .env up -d\n",
    "```\n",
    "\n",
    "You should see output indicating that the containers are starting, including the Ollama container.\n",
    "\n",
    "### 3. Verify that the Ollama container is running:\n",
    "\n",
    "```bash\n",
    "docker ps | grep ollama\n",
    "```\n",
    "\n",
    "You should see output similar to:\n",
    "\n",
    "```\n",
    "3a3b21e26914   ollama/ollama   \"/bin/bash /entrypoint.sh\"   2 minutes ago   Up 2 minutes   11434/tcp, 0.0.0.0:11435->11435/tcp, :::11435->11435/tcp   ragtools_ollama_codestrall\n",
    "```\n",
    "\n",
    "### 4. Check if the Ollama service is responsive:\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11435/api/tags\n",
    "```\n",
    "\n",
    "This should return a JSON response with available models. If it's empty ({}), we need to pull the Codestrall model.\n",
    "\n",
    "### 5. Pull the Codestrall model:\n",
    "\n",
    "If the model isn't already pulled, run:\n",
    "\n",
    "```bash\n",
    "docker exec ragtools_ollama_codestrall ollama pull sammcj/codestral-tweaked-22b\n",
    "```\n",
    "\n",
    "This may take some time depending on your internet connection and system resources.\n",
    "\n",
    "### 6. Test the model with a simple prompt:\n",
    "\n",
    "After the model is pulled, you can test it with:\n",
    "\n",
    "```bash\n",
    "curl -X POST http://localhost:11435/api/generate -d '{\n",
    "  \"model\": \"sammcj/codestral-tweaked-22b\",\n",
    "  \"prompt\": \"Explain what RAG stands for in the context of AI:\"\n",
    "}'\n",
    "```\n",
    "\n",
    "This should return a JSON response containing the generated text explaining RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "### 7. (Optional) Log into the Ollama container:\n",
    "\n",
    "If you need to perform any operations inside the container:\n",
    "\n",
    "```bash\n",
    "docker exec -it ragtools_ollama_codestrall /bin/bash\n",
    "```\n",
    "\n",
    "Once inside, you can run Ollama commands directly:\n",
    "\n",
    "```bash\n",
    "ollama run sammcj/codestral-tweaked-22b \"Explain what RAG stands for in the context of AI:\"\n",
    "```\n",
    "\n",
    "Exit the container when done:\n",
    "\n",
    "```bash\n",
    "exit\n",
    "```\n",
    "\n",
    "### Stopping the Containers\n",
    "\n",
    "When you're done testing, you can stop the Docker containers:\n",
    "\n",
    "```bash\n",
    "docker compose down\n",
    "```\n",
    "\n",
    "By performing these tests, we ensure that our Ollama service is correctly set up and functioning as expected with the sammcj/codestral-tweaked-22b model. The entrypoint script ensures that the Ollama service starts correctly within the container. This setup lays the groundwork for creating our OllamaManager class in the next section, which will provide a more structured way to interact with Ollama in our RAG system.\n",
    "\n",
    "**Note**: If you encounter any issues running these commands, ensure that Docker is properly installed and running on your system, and that you have the necessary permissions to execute Docker commands.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have successfully:\n",
    "\n",
    "1. Set up Ollama instances in Docker containers\n",
    "2. Created an OllamaManager class to handle Ollama operations\n",
    "3. Implemented a method to generate responses from the LLM\n",
    "4. Demonstrated the streaming nature of the LLM's output\n",
    "5. Verified the functionality of our setup with test questions\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Our next notebook will focus on creating a CLI interface for interacting with the LLM. Before diving into the implementation, we'll need to consider:\n",
    "\n",
    "1. LLM Configurables:\n",
    "   - Context length\n",
    "   - Temperature\n",
    "   - Other relevant parameters (e.g., top_p, frequency_penalty, presence_penalty)\n",
    "\n",
    "2. CLI Interface Options:\n",
    "   - Evaluate the merits of adopting a pre-built CLI interface vs. creating our own\n",
    "   - Consider libraries like `click`, `typer`, or `argparse` for building a custom CLI\n",
    "\n",
    "3. Chat Interface Design:\n",
    "   - How to maintain conversation history\n",
    "   - Handling user input and system responses\n",
    "   - Implementing commands for adjusting LLM parameters on-the-fly\n",
    "\n",
    "4. Integration with OllamaManager:\n",
    "   - How to incorporate our existing OllamaManager class into the CLI interface\n",
    "\n",
    "By addressing these points, we'll be well-prepared to create a robust and user-friendly CLI for interacting with our LLM setup."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragtools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
